<h2 align="center">AI coding agent instructions with JUnit test case best practices</h2>

# Table of Contents
1. [Summary](#1-summary)

2. [AI Coding Agent Limitations and Guidance](#2-ai-coding-agent-limitations-and-guidance)

   2.1.  [The Importance of Writing Test Cases According to Best Practices](#21-the-importance-of-writing-test-cases-according-to-best-practices)

   2.2. [Developers as Pilots: AI as Copilot, Not Autopilot](#22-developers-as-pilots-ai-as-copilot-not-autopilot)

3. [Unit Test Cases Best Practices](#3-unit-test-cases-best-practices)

   3.1. [Follow the FIRST Principles](#31-follow-the-first-principles)

   3.2. [Avoid Testing Implementation Details](#32-avoid-testing-implementation-details)

   3.3. [Name Tests for Behavior, Action, and Expected Result](#33-name-tests-for-behavior-action-and-expected-result)

   3.4. [Avoid Logic in Tests](#34-avoid-logic-in-tests)

   3.5. [Use Setup Methods Appropriately (@BeforeEach and @BeforeAll)](#35-use-setup-methods-appropriately-beforeeach-and-beforeall)

4. [Java Junit Coding Agent Instruction File](#4-java-junit-coding-agent-instruction-file)

5. [How to Apply the Instruction File?](#5-how-to-apply-the-instruction-file)

    5.1. [Github Copilot - Why Not Reference the Instructions in `.github/copilot-instructions.md`?](#51-github-copilot---why-not-reference-the-instructions-in-githubcopilot-instructionsmd)

6. [AI Instruction Files Evolve Over Time](#6-ai-instruction-files-evolve-over-time)

7. [Lessons Learned While Using AI Agent to Generate Instructions file](#7-lessons-learned-while-using-ai-agent-to-generate-instructions-file)

8. [References](#8-references)


# 1. Summary
Generative AI models have demonstrated impressive capabilities in software development tasks. **However, to use AIcoding agents effectively within an organization, it is essential to provide them with detailed, unambiguous instructions and to follow prompt engineering best practices.**

**When instructions are vague—such as “write unit test cases” or “install Java version 25” —coding agents may interpret them in different ways. As a result, the generated output may fail to align with the standards, conventions, or workflows established by a team or organization.**

For example, even for a simple task like Java installation, an organization may follow specific conventions such as:
1. **Use Amazon-provided Java** — there are dozens of Java providers available.
2. **Install Java in directories where developers have write permissions.**
3. **Import internal CA certificates provided by the security team** — since multiple approaches exist, specify the exact method your organization uses and provide examples to the coding agent.
4. **Use a specific package manager such as SDKMAN to install Java versions** — include sample installation commands to guide the agent.

**Vague instructions may provide some limited benefit, but to use coding agents properly, detailed andunambiguous instruction files are required.**
**When an organization is in the early stages of adopting AI coding agents, creating instruction files with bestpractices for various tasks will be highly beneficial.**

Examples of tasks that benefit from AI coding agent instruction files include (though many others exist):
1. Best practices and conventions for generating unit test cases.
2. Guidelines for writing integration tests.
3. Language-specific coding best practices for each programming language.

Instruction files tend to become lengthy when following prompt engineering best practices. This is because including sample code in the instructions helps reduce hallucinations by coding agents. A few experienced engineers can writethese AI coding agent instruction files and add them to each Git repository.
Once experienced engineers create and commit these instruction files to the Git repository, **any subsequently generated code will adhere to the conventions and best practices outlined in those files.**

While creating AI coding agent instruction files is a time-consuming process that typically requires experienced engineers, Generative AI models can significantly streamline this task by assisting in drafting and refining theseinstructions.
The remainder of this document provides details on the instruction file for generating unit test cases.

# 2. AI Coding Agent Limitations and Guidance
AI does not possess true intelligence. Instead, it excels at memorizing vast training datasets and simply follows patterns found in the code provided as context, as well as the code on which it was trained. **If the files added to thecontext contain error-prone practices, the code generated by the coding agent will likely replicate those samemistakes. Therefore, it is crucial to guide the coding agent to follow best practices and established guidelines.**

## 2.1 The Importance of Writing Test Cases According to Best Practices
**One of the primary purposes of unit test cases is to ensure that, during code refactoring, well-written unit testscan verify whether the refactored code functions correctly without breaking any of the application's original functionality.**
If unit tests are poorly written (for example, if they test the implementation details rather than the business features),refactoring may cause many unit tests to fail. In such cases, developers must determine whether the failures are due to issues introduced by the refactoring or because the unit tests themselves were not written correctly.
For this reason, adhering to best practices in unit testing is essential.

## 2.2. Developers as Pilots: AI as Copilot, Not Autopilot
While AI coding agent instruction files provide valuable guidance for automated test generation, they do not eliminate the need for developer expertise. **Developers must thoroughly understand testing best practices and remain actively engaged in reviewing AI-generated test cases**. Even with well-configured AI agents, human oversight is critical to ensure test quality, catch edge cases that AI might miss, and verify that generated tests align with the application's business logic and requirements. Treating AI as a productivity tool rather than a replacement for developer judgment ensures that test suites remain reliable, maintainable, and effective.

# 3. Unit Test Cases Best Practices
The Claude AI model was used to research unit-testing best practices, generate example code, and produce the coding-agent instruction document.

## 3.1 Follow the FIRST Principles
**Summary:** Tests should be Fast, Independent/Isolated, Repeatable, Self-validating, and Timely to ensure reliability and maintainability.

### 3.1.1. Best Practices
- **F - Fast:** Execute in milliseconds to encourage frequent runs during development
- **I - Independent/Isolated:** Each test should be completely independent of other tests. Tests shouldn't rely on the execution order or share state. One test's success or failure shouldn't affect another.
  - No shared mutable state between tests
  - Each test sets up its own data
  - Tests can run in any order or in parallel
- **R - Repeatable:** Produce consistent results regardless of environment or execution order - Repeatable
  - Avoid dependencies on:
    - Current date/time (use mocking or fixed values)
    - External systems (databases, APIs - use mocks or in-memory alternatives)
    - Network conditions
    - Random values (use seeded random or fixed values)
- **S - Self-Valididating:** Automatically verify pass/fail without manual inspection
- **T - Timely:** Tests should be written at the right time - ideally just before or alongside the production codeCode Example

### 3.1.2. Code Examples
[Examples with Fast Principle ](examples/follow-FIRST-principles/FastPrincipleExample.java)

[Examples with Independent/Isolated Principle - Good Example](examples/follow-FIRST-principles/IndependentPrincipleGoodExample.java)

[Examples with Independent/Isolated Principle - Bad Example](examples/follow-FIRST-principles/IndependentPrincipleBadExample.java)

[Example with Repeatable Principle - Good Example](examples/follow-FIRST-principles/RepeatablePrincipleGoodExample.java)

[Example with Repeatable Principle - Bad Example](examples/follow-FIRST-principles/RepeatablePrincipleBadExample.java)

[Example with Self-Validation - Good Example](examples/follow-FIRST-principles/SelfValidationPrincipleGoodExample.java)

[Example with Self-Validation - Bad Example](examples/follow-FIRST-principles/SelfValidationPrincipleBadExample.java)


## 3.2. Avoid Testing Implementation Details
**Summary:** Tests using public APIs are resilient to refactoring and won't break when internal implementation changes.
This is fundamental to achieving unchanging tests.

### 3.2.1. Best Practices
1. Test the "what" not the "how"
2. Access the system under test the same way real users would
3. Avoid testing private methods directly. Let private methods be tested implicitly through public method tests
4. Test the complete behavior through the public interface
5. Tests that break during refactoring indicate they weren't written at the appropriate abstraction level

#### 3.2.1.1. Avoid the below (Never Instructions in AI Agents)
1. Test private methods directly
2. Use reflection to access private members for testing
3. Make private methods package-private just for testing

### 3.2.2. Code Examples
[Example for avoid testing implementation details](examples/avoid-testing-implementation-details/AvoidTestImplementationExample.java)


## 3.3. Name Tests for Behavior, Action, and Expected Result
**Summary:** Test names are often the first thing visible in failure reports. Clear names communicate both the action and expected outcome, making debugging faster.

### 3.3.1. Best Practices
1. The test method name should have behavior, actions and expected outcomes
2. Use descriptive names even if verbose
3. Test names serve as documentation
4. Makes test failures immediately understandable
5. Consider starting with "should" to read as a sentence

### 3.3.2. Code Examples
[Example for appropriate names for test methods](examples/name-test-behavior-results/TestNamingConventionExample.java)


## 3.4. Avoid Logic in Tests
**Summary:** Tests should contain minimal logic; complex test logic indicates the test or production code needs
refactoring.

### 3.4.1. Best Practices
1. No conditionals (if/else) in test code
2. No loops in test code
3. Keep test setup simple
4. Complex setup indicates design issues

### 3.4.2. Code Examples
[Example for Avoiding Logic in Tests](examples/avoid-logic-in-tests/AvoidLogicInTestExample.java)


## 3.5. Use Setup Methods Appropriately (@BeforeEach and @BeforeAll)
**Summary:** Leverage JUnit's setup annotations to reduce test duplication while maintaining test independence and
clarity.
Setup methods help initialize common test dependencies and data, but must be used carefully to avoid hidden
dependencies and maintain test readability.

### 3.5.1. Best Practices
1. Use @BeforeEach for per-test setup that ensures each test starts with a fresh state
2. Use @BeforeAll for expensive one-time setup of immutable shared resources
3. Avoid shared mutable state between tests
4. Keep setup methods focused and minimal
5. Document non-obvious setup behavior

### 3.5.2. Code Examples
[Example for Setup Methods Before Each Annotation](examples/use-setup-methods/SetupMethodsBeforeEachExample.java)


[Example for Setup Methods Before All Annotation](examples/use-setup-methods/SetupMethodsBeforeAllExample.java)

## 3.6. Mock External Dependencies
**Summary:** Use mocking frameworks to isolate the unit under test from external systems like AWS (Cloud) Services, databases, APIs, and file systems.

### 3.6.1. Best Practices
1. Mock external services only

   1.1. Message queues/brokers - Examples: Kafka, SQS, SNS, etc.

   1.2. Cache systems - Examples: Redis, Memcached, ElastiCache

   1.3. Third-party libraries that make network calls - Examples: payment gateways, email services, etc.

   1.4. Databases - Examples: DynamoDB, Postgres, MySQL, etc.

   1.5. Cloud storage services - Examples: S3

   1.6. File systems
2. Use frameworks like Mockito for Java
3. Keeps tests fast and reliable
4. Prevents test failures due to external system issues

### 3.6.2. Code Examples
[Example for Mock External Dependencies](examples/mock-external-dependencies/MockExternalDependenciesExample.java)

## 3.7. Use Interface-Based Fake Implementations for Stateful Complex External Dependencies
**Summary:** For complex, stateful external service dependencies, prefer fake implementations over mocking frameworks. Fakes provide realistic behavior, are reusable across tests, and result in more maintainable test suites compared to mocks.

When an external dependency is indirectly used (i.e., not directly injected into the class under test), mocking becomes more challenging. In such cases, interface-based fake implementations provide a simpler and more maintainable testing approach.

Fake implementations are preferred over mocks for complex dependencies. However, fakes require production code to use interface-based design and dependency injection rather than direct method calls. If refactoring production code is feasible, use fakes; otherwise, fall back to mocking frameworks.

### 3.7.1. Best Practices

1. Always prefer interface-based design for testability
2. When refactoring is feasible, prefer fakes over mocks for better maintainability
3. When production code cannot be changed, use Mockito (or PowerMock as last resort)
4. Fakes centralize implementation in one place; mocks scatter configuration across multiple test files

#### 3.7.1.1. Fake Implementation Guidelines
1. **Always prefer interface-based design for testability**
   - Design production code with interfaces from the start to enable fake implementations
   - Use dependency injection to allow swapping real implementations with fakes during testing
   - This approach leads to more maintainable and robust tests

2. **When refactoring production code is feasible, prefer fakes over mocks**
   - If you can refactor production code to use interfaces and dependency injection, fake implementations are strongly preferred
   - Fakes are more maintainable during refactoring since you only update one fake class instead of scattered mock configurations across multiple tests
   - Fakes don't require mocking frameworks and provide realistic behavior

3. **When production code cannot be changed, use mocking frameworks**
   - Use Mockito for standard mocking scenarios (interfaces, regular classes)
   - Use PowerMock only as a last resort for legacy code with static methods, final classes, or private methods that cannot be refactored
   - Note: PowerMock should be viewed as a temporary solution while working toward refactoring the code for better testability


### 3.7.2. Code Examples
[Example for Interface-Based Fake Implementation](examples/fake-impl-interface/FakeImplementationInterfaceExample.java)


## 3.8. Test for Expected Exceptions
**Summary:** Verify that code throws appropriate exceptions for invalid inputs or error conditions. 

### 3.8.1. Best Practices
1. Use assertThrows for exception testing
2. Verify exception type and message
3. Test both happy path and error scenarios

### 3.8.2. Code Examples
[Example Expected Exception](examples/expected-exception/ExpectedExceptionExample.java)


## 3.9. Keep Tests Independent
**Summary:** Tests must not depend on execution order or the results of other tests to ensure reliability.

### 3.9.1. Best Practices
1. Each test should set up its own data
2. Use @BeforeEach for common setup
3. Avoid shared mutable state between tests
4. Tests should pass in any order

### 3.9.2. Code Examples
[Example Keep Tests Independent](examples/keep-independent/KeepTestsIndependentExample.java)



# 4. Java Junit Coding Agent Instruction File
The markdown file below contains instructions for a coding agent to generate JUnit tests. These instructions are derived from the best practices in the previous section using Claude.

[Java Junit Coding Agent Instruction File](.github/instrutions/junit-test-best-practices.instructions.md)

# 5. How to Apply the Instruction File?
Copy the instruction file from the previous section into your Git repository where JUnit test cases need to be created or modified.

An example location for this file is:
```
.github/instructions/junit-test-best-practices.instructions.md
```

Once the instruction file is in place, use the following prompt template with your AI coding agent:

```
Please read and apply all rules from the file:
.github/instructions/junit-test-best-practices.instructions.md

Your task:
1. If JUnit test cases for this class already exist, review them and update, rewrite, or enhance them to fully comply with all best practices in the above file.
2. If no test cases exist, generate comprehensive, high-quality JUnit test cases that follow all rules from the file.

Be explicit about the improvements or changes you are making when modifying existing test cases.

Class under test:
[PASTE YOUR JAVA CLASS HERE]

Existing test file (if applicable):
[PASTE EXISTING TEST CLASS HERE]
```

This prompt template has been tested and works with Claude and GitHub Copilot AI coding agents. It should be compatible with other AI coding agents as well.

While the instruction file can be placed in a local directory such as `~/Library/Application Support/Code/User/prompts/` on macOS (which is specific to Visual Studio Code), this approach makes the file available only on that individual machine and is not shared with other developers.

**Best Practice:** Store the instruction file in your Git repository to ensure all team members have access to the same guidelines and can maintain consistency across the project. 

## 5.1. Github Copilot - Why Not Reference the Instructions in `.github/copilot-instructions.md`?

GitHub Copilot automatically includes the content of `.github/copilot-instructions.md` with every prompt. While you could reference `junit-test-best-practices.instructions.md` from within `copilot-instructions.md` to make it implicitly available, this approach has significant drawbacks.

**The following copilot-instructions.md file has drawbacks and not recommended:**
```
# Global Development Guidelines for Copilot

This repository uses multiple instruction files to maintain consistent development practices.
Refer to the files listed below for guidance on coding standards, testing approaches, design patterns, and other development guidelines.

## Unit Tests
Copilot: Always follow the detailed instructions in: `.github/instructions/junit-test-best-practices.instructions.md`.
```

The drawbacks of the above approach are:

- **Token consumption**: The JUnit best practices instructions would be sent with every prompt, regardless of whether you're writing tests, consuming unnecessary tokens
- **Inefficiency**: These detailed instructions are only needed when generating or modifying JUnit test cases, not for general coding tasks

**Recommendation**: Explicitly reference the JUnit instructions file only when needed (as shown in the prompt template above) rather than including it globally in `copilot-instructions.md`.


# 6. AI Instruction Files Evolve Over Time
Consider an AI agent designed to reply to emails automatically. The agent's instructions evolve over time as humans observe its generated responses and identify opportunities for improvement in specific scenarios, leading them to refine the instructions accordingly.

This represents a key distinction between AI agents and traditional workflow code. In a workflow, once all requirements are known and the code is thoroughly written and tested, the work is complete—no changes are necessary unless requirements change.

**Similarly, just as the email agent's instructions adapt over time, the instructions for AI-generated JUnit test case best practices also evolve and improve iteratively.**

# 7. Lessons Learned While Using AI Agent to Generate Instructions file

The code generated has to be reviewed carefully. The AI generates code for one best practice say naming convention, in other best practices generated, it does not use best practices it recommended.

One best practice suggests test only behaviors and not methods. Another best practice gives an example breaking this best practice - uses method names to test.

The DRY pattern - use it or not to use it: When we ask different AI agents about best practices on Unit test cases, some times it may generate contradicting best practices. For example, Claude generated a best practice to use DRY pattern (do not repeat yourself). While this is useful for general coding, for testing an other design pattern argued that in test cases repetition of the code is acceptable. The author felt that code repetition is fine in unit test cases, otherwise, developers may have to look at multiple helper methods to check the logic in the unit test cases. Well, we need to have a balance of not to repeat too much of common code.

# 8. References
An excellent book on Software Engineering and Engineering Leadership

[Software Engineering at Google: Lessons Learned from Programming Over Time](https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791)