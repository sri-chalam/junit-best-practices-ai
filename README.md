<h2 align="center">AI Coding Agent Instructions file with best practices of Junit Test Cases</h2>

# Summary
Generative AI models have demonstrated impressive capabilities in software development tasks. **However, to use AIcoding agents effectively within an organization, it is essential to provide them with detailed, unambiguous instructions and to follow prompt engineering best practices.**

**When instructions are vague—such as “write unit test cases” or “install Java version 25” —coding agents may interpret them in different ways. As a result, the generated output may fail to align with the standards, conventions, or workflows established by a team or organization.**

For example, even for a simple task like Java installation, an organization may follow specific conventions such as:
1. **Use Amazon-provided Java** — there are dozens of Java providers available.
2. **Install Java in directories where developers have write permissions.**
3. **Import internal CA certificates provided by the security team** — since multiple approaches exist, specify the exact method your organization uses and provide examples to the coding agent.
4. **Use a specific package manager such as SDKMAN to install Java versions** — include sample installation commands to guide the agent.

**Vague instructions may provide some limited benefit, but to use coding agents properly, detailed andunambiguous instruction files are required.**
**When an organization is in the early stages of adopting AI coding agents, creating instruction files with bestpractices for various tasks will be highly beneficial.**

Examples of tasks that benefit from AI coding agent instruction files include (though many others exist):
1. Best practices and conventions for generating unit test cases.
2. Guidelines for writing integration tests.
3. Language-specific coding best practices for each programming language.

Instruction files tend to become lengthy when following prompt engineering best practices. This is because including sample code in the instructions helps reduce hallucinations by coding agents. A few experienced engineers can writethese AI coding agent instruction files and add them to each Git repository.
Once experienced engineers create and commit these instruction files to the Git repository, **any subsequently generated code will adhere to the conventions and best practices outlined in those files.**

While creating AI coding agent instruction files is a time-consuming process that typically requires experienced engineers, Generative AI models can significantly streamline this task by assisting in drafting and refining theseinstructions.
The remainder of this document provides details on the instruction file for generating unit test cases.

# AI Coding Agent Limitations and Guidance
AI does not possess true intelligence. Instead, it excels at memorizing vast training datasets and simply follows patterns found in the code provided as context, as well as the code on which it was trained. **If the files added to thecontext contain error-prone practices, the code generated by the coding agent will likely replicate those samemistakes. Therefore, it is crucial to guide the coding agent to follow best practices and established guidelines.**

## The Importance of Writing Test Cases According to Best Practices
**One of the primary purposes of unit test cases is to ensure that, during code refactoring, well-written unit testscan verify whether the refactored code functions correctly without breaking any of the application's original functionality.**
If unit tests are poorly written (for example, if they test the implementation details rather than the business features),refactoring may cause many unit tests to fail. In such cases, developers must determine whether the failures are due to issues introduced by the refactoring or because the unit tests themselves were not written correctly.
For this reason, adhering to best practices in unit testing is essential.

## Developers as Pilots: AI as Copilot, Not Autopilot
While AI coding agent instruction files provide valuable guidance for automated test generation, they do not eliminate the need for developer expertise. **Developers must thoroughly understand testing best practices and remain actively engaged in reviewing AI-generated test cases**. Even with well-configured AI agents, human oversight is critical to ensure test quality, catch edge cases that AI might miss, and verify that generated tests align with the application's business logic and requirements. Treating AI as a productivity tool rather than a replacement for developer judgment ensures that test suites remain reliable, maintainable, and effective.

# Unit Test Cases Best Practices
The Claude AI model was used to research unit-testing best practices, generate example code, and produce the coding-agent instruction document.

## Follow the FIRST Principles
**Summary:** Tests should be Fast, Independent/Isolated, Repeatable, Self-validating, and Timely to ensure reliability and maintainability.

### Best Practices
- **F - Fast:** Execute in milliseconds to encourage frequent runs during development
- **I - Independent/Isolated:** Each test should be completely independent of other tests. Tests shouldn't rely on the execution order or share state. One test's success or failure shouldn't affect another.
  - No shared mutable state between tests
  - Each test sets up its own data
  - Tests can run in any order or in parallel
- **R - Repeatable:** Produce consistent results regardless of environment or execution order - Repeatable
  - Avoid dependencies on:
    - Current date/time (use mocking or fixed values)
    - External systems (databases, APIs - use mocks or in-memory alternatives)
    - Network conditions
    - Random values (use seeded random or fixed values)
- **S - Self-Valididating:** Automatically verify pass/fail without manual inspection
- **T - Timely:** Tests should be written at the right time - ideally just before or alongside the production codeCode Example

### Code Examples
[Examples with Fast Principle ](examples/follow-FIRST-principles/FastPrincipleExample.java)

[Examples with Independent/Isolated Principle - Good Example](examples/follow-FIRST-principles/IndependentPrincipleGoodExample.java)

[Examples with Independent/Isolated Principle - Bad Example](examples/follow-FIRST-principles/IndependentPrincipleBadExample.java)

[Example with Repeatable Principle - Good Example](examples/follow-FIRST-principles/RepeatablePrincipleGoodExample.java)

[Example with Repeatable Principle - Bad Example](examples/follow-FIRST-principles/RepeatablePrincipleBadExample.java)

[Example with Self-Validation - Good Example](examples/follow-FIRST-principles/SelfValidationPrincipleGoodExample.java)

[Example with Self-Validation - Bad Example](examples/follow-FIRST-principles/SelfValidationPrincipleBadExample.java)


## Avoid Testing Implementation Details
**Summary:** Tests using public APIs are resilient to refactoring and won't break when internal implementation changes.
This is fundamental to achieving unchanging tests.

### Best Practices
1. Test the "what" not the "how"
2. Access the system under test the same way real users would
3. Avoid testing private methods directly. Let private methods be tested implicitly through public method tests
4. Test the complete behavior through the public interface
5. Tests that break during refactoring indicate they weren't written at the appropriate abstraction level

#### Avoid the below (Never Instructions in AI Agents)
1. Test private methods directly
2. Use reflection to access private members for testing
3. Make private methods package-private just for testing

### Code Examples
[Example for avoid testing implementation details](examples/avoid-testing-implementation-details/AvoidTestImplementationExample.java)


## Name Tests for Behavior, Action, and Expected Result
**Summary:** Test names are often the first thing visible in failure reports. Clear names communicate both the action and expected outcome, making debugging faster.

### Best Practices
1. The test method name should have behavior, actions and expected outcomes
2. Use descriptive names even if verbose
3. Test names serve as documentation
4. Makes test failures immediately understandable
5. Consider starting with "should" to read as a sentence

### Code Examples
[Example for appropriate names for test methods](examples/name-test-behavior-results/TestNamingConventionExample.java)


## Avoid Logic in Tests
**Summary:** Tests should contain minimal logic; complex test logic indicates the test or production code needs
refactoring.

### Best Practices
1. No conditionals (if/else) in test code
2. No loops in test code
3. Keep test setup simple
4. Complex setup indicates design issues

### Code Examples
[Example for Avoiding Logic in Tests](examples/avoid-logic-in-tests/AvoidLogicInTestExample.java)


## Use Setup Methods Appropriately (@BeforeEach and @BeforeAll)
**Summary:** Leverage JUnit's setup annotations to reduce test duplication while maintaining test independence and
clarity.
Setup methods help initialize common test dependencies and data, but must be used carefully to avoid hidden
dependencies and maintain test readability.

### Best Practices
1. Use @BeforeEach for per-test setup that ensures each test starts with a fresh state
2. Use @BeforeAll for expensive one-time setup of immutable shared resources
3. Avoid shared mutable state between tests
4. Keep setup methods focused and minimal
5. Document non-obvious setup behavior

### Code Examples
[Example for Setup Methods Before Each Annotation](examples/use-setup-methods/SetupMethodsBeforeEachExample.java)


[Example for Setup Methods Before All Annotation](examples/use-setup-methods/SetupMethodsBeforeAllExample.java)

